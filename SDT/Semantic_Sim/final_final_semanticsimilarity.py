# -*- coding: utf-8 -*-
"""Final_Final_SemanticSimilarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BhmtliKaoQyBUHCYM_rCjQ09ccFTeFV2

**Semantic Similarity Scores**

---



This is a colab notebook to obtain the cosine similarity scores for 200 random words, chosen from words from the Google News Dataset.

- Setting up the gensim module from NLTK
"""

from nltk.test.gensim_fixt import setup_module
setup_module()

"""- Importing all the necessary modules"""

import numpy as np
import pandas as pd
import gensim
import csv
import random

"""In this notebook, we utilise a pre-trained model based on 10 billion words from the google news dataset

- After loading the pre-trained model, we obtain a list of the unique words using the dot vocab method
"""

from nltk.data import find
import nltk
nltk.download('word2vec_sample')
word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))
word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)

# contains the list of all unique words in pre-trained word2vec vectors
w2v_vocabulary = word2vec.vocab

"""- Having obtained this list, we then move to choosing the first 200 words to implement our methods on them"""

count=0
w2v_chosenwords=[]
for i in w2v_vocabulary:
  w2v_chosenwords.append(i)
  count+=1
  if count>200:
    break
print("The chosen words:",w2v_chosenwords)

"""*Information to note: Dimension of the word vectors of the words is 300, which needs to be noted*

Defining a few functions, which are necessary for the problem:
- Mean & Standard Deviation calculator
- Random Pair Generator
"""

def mean_var(arr):
  temp=arr.shape
  if len(temp)==1:
    sum=0
    for i in range(temp[0]):
      sum+=arr[i]
    mean=sum/temp[0]
    var=0
    for i in range(temp[0]):
      var+=(arr[i]-mean)**2
    std=(var/temp[0])**0.5
    return (mean,std)

def random_pairs(arr):
    x=[] 
    for j in range(int(len(arr)/2)):
      x.append([arr[i] for i in random.sample(range(len(arr)), 2)])
    return x

"""Beyond this, we need to generate a dataframe or a csv file with columns as -
- Serial Number
- word2vec similarity
- Dot Product calculated similarity
- PCA value similarity(dimension=4)
- t-SNE value similarity
- Newly Proposed method similarity
"""

dataframe = pd.DataFrame(columns =['Serial Number'], dtype = int)

dataframe

"""*To choose n/2 pairs randomly from the chosen n sample points*"""

w2v_randompairs=random_pairs(w2v_chosenwords)

print(w2v_randompairs)

"""Similarity Values are values calculated as cosine similarity between the vectors of two words, which means a total of 100 Similarity values will be generated from the 100 pairs"""

serial=[]
for i in range(1,101):
  serial.append(i)
dataframe['Serial Number']=serial

dataframe

"""**Method 1:**
The first column is to be filled with the recorded word2vec values, which will be used as a standard for comparison
"""

w2v_values=[]
for i in w2v_randompairs:
  w2v_values.append(word2vec.wv.similarity(i[0],i[1]))
dataframe['w2v_idealvalues']=w2v_values

dataframe

"""**Method 2:**
The second column is to be filled with the dot products of two word vectors, to compare the closeness of the values
"""

dotprod_values=[]
for i in w2v_randompairs:
  vector_1=word2vec.wv[i[0]]
  vector_2=word2vec.wv[i[1]]
  value_calc=np.dot(vector_1,vector_2)
  dotprod_values.append(value_calc)
dataframe['dotprod_calcvalues']=dotprod_values

dataframe

"""**Method 3:**
The third column is to be filled with values obtained from the word vectors after **Principal Component Analysis**, with reduced dimensions being restricted to 4
"""

count = 0
max_count = 200
X = np.zeros(shape=(300,max_count))
X=X.T

c=0
for i in w2v_randompairs:
  X[c,:]=(word2vec.wv[i[0]])
  X[c+1,:]=(word2vec.wv[i[1]])
  c+=1
  if c>198:
    break
print(X)
X.shape

"""- Implement PCA from the scikit library"""

from sklearn.decomposition import PCA
pca = PCA(n_components=4)
X_4 = pca.fit_transform(X)

X_4.shape

pca_values=[]
for i in range(0,X_4.shape[0]-1,2):
  pca_vector1=(X_4[i,:])
  pca_vector2=(X_4[i+1,:])
  pca_value=np.dot(pca_vector1,pca_vector2)
  pca_values.append(pca_value)

dataframe['pca_values']=pca_values

dataframe

"""**Method 4:**
The fourth column is to be filled with values obtained from the word vectors after **t-SNE** is applied, with reduced dimensions being restricted to 3
"""

from sklearn.manifold import TSNE
t_sne = TSNE(n_components=3, learning_rate='auto',init='random')
X_embedded= t_sne.fit_transform(X)

X_embedded.shape

tSNE_values=[]
for i in range(0,X_embedded.shape[0]-1,2):
  tSNE_vector1=(X_embedded[i,:])
  tSNE_vector2=(X_embedded[i+1,:])
  tSNE_value=np.dot(tSNE_vector1,tSNE_vector2)
  tSNE_values.append(tSNE_value)

dataframe['tSNE_values']=tSNE_values
dataframe

"""*Temporary Checkpoint - downloading the dataframe*"""

dataframe.to_csv('tablecheck.csv')

"""**Method 5:**
The fifth column is to be filled with values obtained from the word vectors after the proposed method is applied.

Consideration:
- **X has dimensions (200,300)** following which:
  - Calculate Mean and Standard Deviation, of each column as C(mu,j) and C(sigma,j).
  - Convert each element C(ij) of a column to a normalised form 
     C'(ij)= [C(ij)-C(mu,j)]/C(sigma,j)
"""

mean_varlist=[]
for i in range(300):
  arr=X[:,i]
  mean_varlist.append(mean_var(arr))

print(mean_varlist)

len(mean_varlist)

copy=X.copy()

for i in range(300):
  arr=X[:,i]
  mean_vartup=mean_varlist[i]
  arr=(arr-mean_vartup[0])/(mean_vartup[1])
  copy[:,i]=arr
print(copy)
print(copy.shape)

type(X)

print(X)

"""copy is the normalized X matrix

- Calculation of Moments is necessary:
  - For an n-th moment, each element along a row, should be raised to the power n.
  - Beyond this, all the rows elements are added along a column, followed by average of the 200 elements.
"""

def moment(n,copy):
  copy_new=copy.copy()
  for i in range(200):
    for j in range(300):
      copy_new[i,j]=copy[i,j]**n
  temp=[]
  for i in range(300):
    temp.append(np.sum(copy_new[:,i])/200)
  return temp

moment_0=moment(0,copy)
print(moment_0)

moment_1=moment(1,copy)
print(moment_1)

moment_2=moment(2,copy)
print(moment_2)

moment_3=moment(3,copy)
print(moment_3)

moment_4=moment(4,copy)
print(moment_4)

moment_5=moment(5,copy)
print(moment_5)

moment_6=moment(6,copy)
print(moment_6)

moment_7=moment(7,copy)
print(moment_7)

moment_8=moment(8,copy)
print(moment_8)

moment_9=moment(9,copy)
print(moment_9)

moment_10=moment(10,copy)
print(moment_10)

moment_matrix=np.zeros(shape=(300,11))

moment_array=[moment_0,moment_1,moment_2,moment_3,moment_4,moment_5,moment_6,moment_7,moment_8,moment_9,moment_10]

for i in range(11):
  moment_matrix[:,i]=moment_array[i]
print(moment_matrix)

"""Hence, the matrix consisting of moments from 0 to 10, are obtained. This is known as **basis matrix**"""

mean_varlist_new=[]
for i in range(11):
  arr_new=moment_matrix[:,i]
  mean_varlist_new.append(mean_var(arr_new))

print(mean_varlist_new)
copy_3=moment_matrix.copy()

for i in range(10):
  arr_one=moment_matrix[:,i+1]
  mean_vartupnew=mean_varlist_new[i+1]
  arr_one=(arr_one-mean_vartupnew[0])/(mean_vartupnew[1])
  copy_3[:,i+1]=arr_one
print(copy_3)
print(copy_3.shape)

"""The next step:
- W which is the compressed word vector, needs to be obtained for every word, using the pseudo inverse matrix method.
 ================> W = ((X.T)X)^(-1)) (X.T)Y
- Y is the word vector of each word and X is the matrix of every word

- **moment_matrix** - (300,11)
- **Y** - (300,1)
"""

word_vecmatrix=[]
for i in range(200):
  Y=X[i,:]
  moment_matrixsquare=np.matmul(copy_3.T,copy_3)
  moment_inverse=np.linalg.inv(moment_matrixsquare)
  pre_mult=np.matmul(moment_inverse,copy_3.T)
  W=np.matmul(pre_mult,Y)
  word_vecmatrix.append(W)
print(word_vecmatrix)

len(word_vecmatrix)

newmethod_values=[]
count=0
for i in range(0,199,2):
  newmethod_vec1=word_vecmatrix[i]
  newmethod_vec2=word_vecmatrix[i+1]
  newmethod_value=np.dot(newmethod_vec1,newmethod_vec2)
  newmethod_values.append(newmethod_value)
print(newmethod_values)

print(len(newmethod_values))

dataframe['newmethod_values']=newmethod_values

dataframe

"""*Final Checkpoint- Table consists of similarity values of all methods*"""

dataframe.to_csv('finaltable_new.csv')

"""Plot the moment_matrix before normalising
- Calculate 7th column with new method without normalising

"""
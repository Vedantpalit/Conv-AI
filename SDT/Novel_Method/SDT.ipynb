{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Install \n",
        "#1) Representation Function to Obtain Language Token Vectors\n",
        "#2) Graphviz for visualization"
      ],
      "metadata": {
        "id": "BwkFEaWG3y8f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0CLnqHx3ZiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5bffe69-21d9-408a-e619-dc876d2e04ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting simpletransformers\n",
            "  Downloading simpletransformers-0.63.9-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.5/250.5 KB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from simpletransformers) (1.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from simpletransformers) (1.3.5)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb>=0.10.32\n",
            "  Downloading wandb-0.13.7-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers>=4.6.0\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from simpletransformers) (1.21.6)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.16.0-py2.py3-none-any.whl (9.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from simpletransformers) (1.7.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 KB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.8/dist-packages (from simpletransformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from simpletransformers) (2.25.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from simpletransformers) (2022.6.2)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (from simpletransformers) (2.9.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.6.0->simpletransformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.6.0->simpletransformers) (3.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.6.0->simpletransformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb>=0.10.32->simpletransformers) (3.19.6)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb>=0.10.32->simpletransformers) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb>=0.10.32->simpletransformers) (7.1.2)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m823.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb>=0.10.32->simpletransformers) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb>=0.10.32->simpletransformers) (57.4.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.12.1-py2.py3-none-any.whl (174 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.3/174.3 KB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->simpletransformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->simpletransformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->simpletransformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->simpletransformers) (2.10)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets->simpletransformers) (2022.11.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->simpletransformers) (3.8.3)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets->simpletransformers) (0.3.6)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->simpletransformers) (9.0.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->simpletransformers) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->simpletransformers) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->simpletransformers) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->simpletransformers) (1.2.0)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.8/dist-packages (from streamlit->simpletransformers) (6.0.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from streamlit->simpletransformers) (7.1.2)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-2.2.1-py3-none-manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.0/79.0 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semver\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from streamlit->simpletransformers) (4.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.8/dist-packages (from streamlit->simpletransformers) (5.2.0)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.8/dist-packages (from streamlit->simpletransformers) (1.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from streamlit->simpletransformers) (4.4.0)\n",
            "Collecting rich>=10.11.0\n",
            "  Downloading rich-13.0.1-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.1/238.1 KB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blinker>=1.0.0\n",
            "  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
            "Collecting validators>=0.2\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.8/dist-packages (from streamlit->simpletransformers) (5.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard->simpletransformers) (1.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->simpletransformers) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->simpletransformers) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->simpletransformers) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->simpletransformers) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard->simpletransformers) (3.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard->simpletransformers) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->simpletransformers) (2.15.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->simpletransformers) (1.51.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.11.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.12.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.10.32->simpletransformers) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->simpletransformers) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->simpletransformers) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->simpletransformers) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->simpletransformers) (6.0.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=1.4->streamlit->simpletransformers) (3.11.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers>=4.6.0->simpletransformers) (3.0.9)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.11.0->streamlit->simpletransformers) (2.6.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from validators>=0.2->streamlit->simpletransformers) (4.4.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (2.0.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (0.19.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (5.10.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (3.2.2)\n",
            "Building wheels for collected packages: seqeval, validators, pathtools\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=a242c03060cca01b7f69f3e8a7bc91a73bb568117ff7d5b7346d5287cbe266be\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19581 sha256=96e5b6238c0f7dc89bf61d83c0ac7f3c8967d58b08b0dd983d47cba1111b0cc9\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/09/72/3eb74d236bb48bd0f3c6c3c83e4e0c5bbfcbcad7c6c3539db8\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=290b99bdbb19e8c2f5a96e6d621b7f5dba470e9123cc44a4ed89db96d8150436\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built seqeval validators pathtools\n",
            "Installing collected packages: tokenizers, sentencepiece, pathtools, commonmark, xxhash, watchdog, validators, urllib3, smmap, shortuuid, setproctitle, semver, rich, pympler, multiprocess, docker-pycreds, blinker, sentry-sdk, pydeck, gitdb, seqeval, responses, huggingface-hub, GitPython, wandb, transformers, streamlit, datasets, simpletransformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed GitPython-3.1.30 blinker-1.5 commonmark-0.9.1 datasets-2.8.0 docker-pycreds-0.4.0 gitdb-4.0.10 huggingface-hub-0.11.1 multiprocess-0.70.14 pathtools-0.1.2 pydeck-0.8.0 pympler-1.0.1 responses-0.18.0 rich-13.0.1 semver-2.13.0 sentencepiece-0.1.97 sentry-sdk-1.12.1 seqeval-1.2.2 setproctitle-1.3.2 shortuuid-1.0.11 simpletransformers-0.63.9 smmap-5.0.0 streamlit-1.16.0 tokenizers-0.13.2 transformers-4.25.1 urllib3-1.26.13 validators-0.20.0 wandb-0.13.7 watchdog-2.2.1 xxhash-3.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.8/dist-packages (0.10.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install simpletransformers\n",
        "!pip install graphviz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries -- torch and Submodules"
      ],
      "metadata": {
        "id": "dFj6qln34IS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD"
      ],
      "metadata": {
        "id": "LecU6Ffq4JtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Cuda Available and Assign Device to Perform Tensor Operations"
      ],
      "metadata": {
        "id": "75FTm2X94NXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "Cepd9tr64PuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess String Function -- White Spaces and Lowercase Alphabets Only"
      ],
      "metadata": {
        "id": "GKTMzint4mq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from string import ascii_lowercase as allowed_chars\n",
        "\n",
        "def clean_sentence(sentence):\n",
        "    \n",
        "    lowercase = sentence.lower()\n",
        "    chars = list(lowercase)\n",
        "    clean_sentence = (''.join([char for char in chars if char in list(allowed_chars)+[' ']])).strip()\n",
        "    return clean_sentence"
      ],
      "metadata": {
        "id": "65Z3ZL_Y409L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sufficiently Descriptive Transformer Class"
      ],
      "metadata": {
        "id": "mgI-3Teq5ZEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from simpletransformers.language_representation import RepresentationModel\n",
        "\n",
        "class SDT(object):\n",
        "\n",
        "  #class constructor\n",
        "  #initialize number of heads\n",
        "  #initialize number of blocks\n",
        "  #initialize order of moments\n",
        "  #initialize model parameters\n",
        "  def __init__(self,\n",
        "               N_heads = 1,\n",
        "               N_blocks = 1,\n",
        "               order = 4,\n",
        "               check_graph = False):\n",
        "    \n",
        "    self.repr_model = RepresentationModel(model_type=\"bert\",\n",
        "                                          model_name=\"bert-base-uncased\",\n",
        "                                          use_cuda=(device if device == 'cuda' else False))\n",
        "    \n",
        "    self.N_heads = N_heads\n",
        "    self.N_blocks = N_blocks\n",
        "    self.order = order\n",
        "    self.check_graph = check_graph\n",
        "\n",
        "    self.repr_size = len(self.repr_model.encode_sentences([\"dummy\"],combine_strategy=\"mean\")[0])\n",
        "    self.M = int(self.repr_size/self.N_heads) #head representation size\n",
        "\n",
        "    self.Wq = [torch.rand(self.M,self.M,requires_grad=True) for i in range(self.N_heads)] #optional (self.M x m < self.M)\n",
        "    self.Wk = [torch.rand(self.M,self.M,requires_grad=True) for i in range(self.N_heads)]\n",
        "    self.Wv = [torch.rand(self.M,self.M,requires_grad=True) for i in range(self.N_heads)]\n",
        "    \n",
        "    self.W = [torch.rand(self.order,self.order,requires_grad=True) for i in range(self.N_heads)]\n",
        "\n",
        "  #graph computation\n",
        "  def compute_graph(self,\n",
        "                    Input):\n",
        "    \n",
        "    #checking graph signal for Input\n",
        "    Input_tokens = Input.split(' ')\n",
        "    graph = {}\n",
        "    n_tokens = len(Input_tokens)\n",
        "        \n",
        "    #compute function value for node-edge-node triplets\n",
        "    for i in range(n_tokens):\n",
        "      for j in range(n_tokens):\n",
        "\n",
        "        token_i = Input_tokens[i]\n",
        "        token_j = Input_tokens[j]\n",
        "          \n",
        "        token_i_id = token_i+str(i)\n",
        "        token_j_id = token_j+str(j) \n",
        "\n",
        "        token_i_v = round(float(obj.forward(token_i)),2)\n",
        "        token_j_v = round(float(obj.forward(token_j)),2)\n",
        "\n",
        "        edge_weight = 1.0 - round(float(token_i_v - token_j_v),2)\n",
        "          \n",
        "        if ((token_j_id,token_i_id) in graph) or token_i_id == token_j_id:\n",
        "          continue\n",
        "\n",
        "        graph[(token_i_id,token_j_id)] = (str(token_i_v),str(edge_weight),str(token_j_v))\n",
        "\n",
        "    self.graph = graph\n",
        "\n",
        "  #parameter update equations\n",
        "  def update_params(self,\n",
        "                    learning_rate):\n",
        "    \n",
        "    for i in range(self.N_heads):\n",
        "      \n",
        "      self.Wq[i] += learning_rate * self.Wq[i].grad\n",
        "      self.Wk[i] += learning_rate * self.Wk[i].grad\n",
        "      self.Wv[i] += learning_rate * self.Wv[i].grad\n",
        "      self.W[i] += learning_rate * self.W[i].grad\n",
        "\n",
        "  #zero gradients after each gradient update\n",
        "  def zero_grad(self):\n",
        "      \n",
        "    for i in range(self.N_heads):\n",
        "\n",
        "      self.Wq[i].grad.zero_()\n",
        "      self.Wk[i].grad.zero_()\n",
        "      self.Wv[i].grad.zero_()\n",
        "      self.W[i].grad.zero_()\n",
        "\n",
        "  #Batch Normalization\n",
        "  def normalize(self,X):\n",
        "      \n",
        "    mu = X.mean(dim=0,keepdim=True)\n",
        "    sigma = X.std(dim=0,keepdim=True)\n",
        "    Z = torch.nan_to_num((X-mu)/sigma)\n",
        "    return Z\n",
        "\n",
        "  #Compute k-th order moments\n",
        "  def compute_moments(self,\n",
        "                      embeddings):\n",
        "      \n",
        "    #Center the Dataset first\n",
        "    X = embeddings\n",
        "    Z = self.normalize(X)\n",
        "\n",
        "    #compute upto kth order moments, count , mean , std-dev, skewness, kurtoisis, etc. ..\n",
        "    Z.to(device)\n",
        "    moments = []\n",
        "    for k in range(self.order):\n",
        "      moment_k = torch.pow(Z,k).mean(dim=0,keepdim=True)\n",
        "      moments.append(moment_k[0])\n",
        "    moments = torch.column_stack(moments)\n",
        "\n",
        "    #re-center\n",
        "    moments = self.normalize(moments)\n",
        "\n",
        "    #return normalized decentralized moments\n",
        "    return moments\n",
        "\n",
        "  #Obtain Input Embeddings\n",
        "  def embed_input(self,\n",
        "                  Input):\n",
        "\n",
        "    clean_Input = clean_sentence(Input)\n",
        "    head_embeddings,tokens = [],clean_Input.split(' ')\n",
        "\n",
        "    for i in range(self.N_heads):\n",
        "\n",
        "      embeddings = []\n",
        "      for token in tokens:\n",
        "        embeddings.append(self.repr_model.encode_sentences([token],combine_strategy=\"mean\")[0])      \n",
        "        \n",
        "      head_embeddings.append(torch.stack([torch.tensor(embedding[(i*self.M):((i+1)*(self.M))]) for embedding in embeddings]))\n",
        "\n",
        "    head_embeddings = torch.stack(head_embeddings)\n",
        "    return head_embeddings\n",
        "\n",
        "  #Aggregate Forward Pass\n",
        "  def aggregate(self,\n",
        "                Input):\n",
        "\n",
        "    head_embeddings = self.embed_input(Input)\n",
        "      \n",
        "    for i in range(self.N_blocks):\n",
        "\n",
        "      #compute moments\n",
        "      head_embeddings = torch.stack([self.compute_moments(head_embeddings[i]).t() for i in range(self.N_heads)])\n",
        "        \n",
        "      #compute Q,K,V for all attention heads\n",
        "      head_Qs = torch.stack([head_embeddings[i] @ self.Wq[i] for i in range(self.N_heads)])\n",
        "      head_Ks = torch.stack([head_embeddings[i] @ self.Wk[i] for i in range(self.N_heads)])\n",
        "      head_Vs = torch.stack([head_embeddings[i] @ self.Wv[i] for i in range(self.N_heads)])\n",
        "\n",
        "      #compute softmax(QK^T/sqrt_d)\n",
        "      sqrt_d = torch.sqrt(torch.tensor(self.M))\n",
        "      softmax = nn.Softmax(dim=1)\n",
        "      head_QKs = torch.stack([softmax(torch.div(head_Qs[i] @ head_Ks[i].t(),sqrt_d)) for i in range(self.N_heads)])\n",
        "\n",
        "      #compute weighted residual and Vs\n",
        "      residuals = torch.stack([head_QKs[i] @ head_Vs[i] for i in range(self.N_heads)])\n",
        "      head_Vs = torch.stack([residuals[i] + head_embeddings[i] for i in range(self.N_heads)])\n",
        "    \n",
        "      #batch normalize\n",
        "      head_Vs = torch.stack([self.normalize(head_Vs[i]) for i in range(self.N_heads)])\n",
        "      head_embeddings = head_Vs\n",
        "\n",
        "    assoc_matrix = torch.stack([self.W[i] @ (head_embeddings[i] @ head_embeddings[i].t()) for i in range(self.N_heads)])\n",
        "    mean_assoc_matrix = torch.mean(assoc_matrix,dim=0)\n",
        "      \n",
        "    return torch.sum(mean_assoc_matrix)\n",
        "\n",
        "  #Forward Pass\n",
        "  def forward(self,\n",
        "              Input):\n",
        "\n",
        "    output = self.aggregate(Input)\n",
        "    #outputs = torch.stack([self.aggregate(token) for token in Input.split(' ')])\n",
        "    #output += torch.sum(outputs @ outputs.t())\n",
        "    return torch.sigmoid(output)"
      ],
      "metadata": {
        "id": "r0-_Z0tN5gQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unit Tests"
      ],
      "metadata": {
        "id": "d3j3lSBdS_fF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import sample\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "#generating a pickle file\n",
        "pickle_object=open('Novel_Method.pkl','ab')\n",
        "#data and counts\n",
        "data = [[\"a golden retriever is a dog\",1],\n",
        "        [\"a cat is a dog\",0]]*2\n",
        "\n",
        "n_data = len(data)\n",
        "\n",
        "#seed random number generator\n",
        "torch.manual_seed(0)\n",
        "\n",
        "#SDTconfig\n",
        "#n_heads = 1 #-->1-12\n",
        "#n_blocks = 1 #-->1-12\n",
        "#order = 4 #-->0-50\n",
        "check_graph = False\n",
        "for n_heads_i in range(1,13):\n",
        "  for n_blocks_i in range(1,13):\n",
        "    for order_i in range(1,51):\n",
        "      #obtain SDT object\n",
        "      obj = SDT(N_heads = n_heads_i,\n",
        "                N_blocks = n_blocks_i,\n",
        "                order = order_i,\n",
        "                check_graph = check_graph)\n",
        "\n",
        "      #training config\n",
        "\n",
        "      learning_rate =[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,1] #-->0.001,0.002,...0.009, 0.01,0.02,...0.09,0.1,...0.9,1.0\n",
        "      n_epochs = [10,50,100,200,500] #-->10,50,100,200,500\n",
        "      batch_size = [0.1,0.2,0.5,0.8,1.0] #-->0.1,0.2,0.5,0.8,1.0\n",
        "      batch_size = [1.0]\n",
        "\n",
        "      #training loop\n",
        "      for learning_rate_i in learning_rate:\n",
        "        for batch_size_i in batch_size:\n",
        "          for n_epochs_i in n_epochs:\n",
        "            for i in tqdm(range(n_epochs_i)):\n",
        "\n",
        "              #initialize loss\n",
        "              loss = torch.tensor(0.0)\n",
        "\n",
        "              #sample batch\n",
        "              batch = sample(data,int(batch_size_i*n_data))\n",
        "              n_batch = len(batch)\n",
        "\n",
        "              #calculate total loss for batch\n",
        "              for i in range(n_batch):\n",
        "\n",
        "                Input = batch[i][0]\n",
        "                label = batch[i][1]\n",
        "\n",
        "                g_truth = torch.tensor(([1.0, 0.0] if label == 1 else [0.0, 1.0]))\n",
        "                output = obj.forward(Input) \n",
        "                output_probs = torch.stack([output, \n",
        "                                            1-output])\n",
        "                    \n",
        "                output_loss = -1 * (g_truth @ torch.log(output_probs))\n",
        "                loss += output_loss\n",
        "\n",
        "              #average loss for the batch\n",
        "              loss /= n_batch\n",
        "\n",
        "              #print loss\n",
        "              #print (loss)\n",
        "\n",
        "              #compute gradients\n",
        "              loss.backward()\n",
        "\n",
        "              #update parameters\n",
        "              with torch.no_grad():\n",
        "                obj.update_params(learning_rate_i)\n",
        "\n",
        "              #zero grads out\n",
        "              obj.zero_grad()\n",
        "\n",
        "              #pickle the obj file after each epoch and write to disk\n",
        "              pickle.dumps(obj,protocol=None, fix_imports=True, buffer_callback=pickle_object.write)\n",
        "\n",
        "if obj.check_graph:\n",
        "\n",
        "  for i in range(n_data):\n",
        "\n",
        "    Input = data[i][0]\n",
        "    obj.compute_graph(Input)\n",
        "    Input_graph = obj.graph"
      ],
      "metadata": {
        "id": "CSI4XFK_TBKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graph Visualization"
      ],
      "metadata": {
        "id": "Ja8FyZtAnns3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Graph,Source\n",
        "\n",
        "def visualize(Input_graph):\n",
        "\n",
        "  g = Graph('G', filename='process.gv', engine='sfdp')\n",
        "\n",
        "  for item in Input_graph:\n",
        "\n",
        "    g.edge(item[0],item[1],label=Input_graph[item][1])\n",
        "\n",
        "  return g\n",
        "\n",
        "visualize(Input_graph)"
      ],
      "metadata": {
        "id": "joSttG4rnqyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5R50HP0o2QfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(obj.repr_model.encode_sentences([\"I am a dog\"],combine_strategy=\"mean\")[0])\n",
        "y = torch.tensor(obj.repr_model.encode_sentences([\"I am a cat\"],combine_strategy=\"mean\")[0])\n",
        "x @ y"
      ],
      "metadata": {
        "id": "oMAfA1qD_HNl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}